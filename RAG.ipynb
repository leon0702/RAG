{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from LangChain and related packages\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# relative_path = 'D:\\Work\\Python_Code' # For folder loader\n",
    "relative_path = 'D:\\Work\\Python_Code\\\\nke-10k-2023.txt' # One file loader\n",
    "\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "##################################################\n",
    "### For folder loader\n",
    "# loader = DirectoryLoader(relative_path, glob=\"**/*.txt\", show_progress=True,loader_cls=TextLoader, loader_kwargs=text_loader_kwargs) \n",
    "# docs = []\n",
    "# for loader in loaders:\n",
    "#     docs.extend(loader.load())\n",
    "##################################################\n",
    "loader = TextLoader(relative_path,autodetect_encoding = True)\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = 256,\n",
    "    chunk_overlap  = 20\n",
    "    )\n",
    "spl_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "#### Most Download model\n",
    "modelPath = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#### BU3 text2vec\n",
    "# modelPath = \"shibing624/text2vec-base-chinese\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Virtual_Enviroment\\rag\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://gpt-east-usa.openai.azure.com/ to https://gpt-east-usa.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "d:\\Work\\Virtual_Enviroment\\rag\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "d:\\Work\\Virtual_Enviroment\\rag\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://gpt-east-usa.openai.azure.com/ to https://gpt-east-usa.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the AzureChatOpenAI model from LangChain for chat-based interactions using Azure's OpenAI service\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an instance of AzureChatOpenAI, configuring it to use specific settings for the OpenAI API\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=\"\",  # Replace with your Azure OpenAI API base URL (e.g., \"https://<region>.openai.azure.com/\")\n",
    "    openai_api_version=\"2024-04-01-preview\",  # Specifies the API version for Azure OpenAI\n",
    "    azure_deployment=\"gpt-4o\",  # Deployment name on Azure; replace with your specific model deployment name\n",
    "    openai_api_key=\"\",  # Replace with your Azure OpenAI API key\n",
    "    openai_api_type=\"azure\",  # Specifies that the API type is 'azure'\n",
    "    temperature=0.5,  # Controls randomness in responses; 0.5 provides balanced responses\n",
    "    max_tokens=1024,  # Sets the maximum number of tokens in the response (controls response length)\n",
    "    streaming=True  # Enables streaming for real-time response generation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Retriever\n",
    "The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm.\n",
    "\n",
    "By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single algorithm.\n",
    "\n",
    "The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as \"hybrid search\". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary retriever and vector store modules from LangChain and LangChain Community\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize the BM25 retriever using preprocessed text documents\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    spl_text  # Replace 'spl_text' with your list of documents or text segments\n",
    ")\n",
    "bm25_retriever.k = 3  # Set the number of top results to retrieve with BM25\n",
    "\n",
    "# Initialize the FAISS vector store retriever\n",
    "embedding = embeddings  # Replace 'embeddings' with your initialized embedding model\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    spl_text, embedding  # Store documents in FAISS with embeddings\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 results based on similarity\n",
    "\n",
    "# Initialize the ensemble retriever to combine the BM25 and FAISS retrievers with equal weight\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],  # List of retrievers to combine\n",
    "    weights=[0.5, 0.5]  # Set weights for each retriever in the ensemble (e.g., 50% for each)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Encoder Reranker\n",
    "This notebook shows how to implement reranker in a retriever with your own cross encoder from Hugging Face cross encoder models or Hugging Face models that implements cross encoder function (example: BAAI/bge-reranker-base). SagemakerEndpointCrossEncoder enables you to use these HuggingFace models loaded on Sagemaker.\n",
    "\n",
    "This builds on top of ideas in the ContextualCompressionRetriever. Overall structure of this document came from Cohere Reranker documentation.\n",
    "\n",
    "For more about why cross encoder can be used as reranking mechanism in conjunction with embeddings for better retrieval, refer to Hugging Face Cross-Encoders documentation.\n",
    "\n",
    "# Doing reranking with CrossEncoderReranker\n",
    "Now let's wrap our base retriever with a ContextualCompressionRetriever. CrossEncoderReranker uses HuggingFaceCrossEncoder to rerank the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D:\\\\Work\\\\Python_Code\\\\nke-10k-2023.txt'}, page_content='| ITEM 16. Form 10-K Summary |  | 97 |\\n| Signatures |  | 99 |\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this'), Document(metadata={'source': 'D:\\\\Work\\\\Python_Code\\\\nke-10k-2023.txt'}, page_content='NIKE Air-Sole cushioning components. During fiscal 2023, Air Manufacturing Innovation, a wholly-owned subsidiary, with facilities\\nnear Beaverton, Oregon, in Dong Nai Province, Vietnam, and St. Charles, Missouri, as well as contract manufacturers in China'), Document(metadata={'source': 'D:\\\\Work\\\\Python_Code\\\\nke-10k-2023.txt'}, page_content='and 61% for fiscal 2022 and fiscal 2021, respectively. We sell our products to retail accounts through our own NIKE Direct\\noperations and through a mix of independent distributors, licensees and sales representatives around the world. We sell to')]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for contextual compression and cross-encoder reranking\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# Initialize the cross-encoder model for reranking\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")  # Load a cross-encoder reranker model\n",
    "\n",
    "# Initialize the compressor with the cross-encoder model and set the top_n parameter\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)  # Keep the top 3 documents based on relevance\n",
    "\n",
    "# Initialize the compression retriever, combining the compressor and a base retriever (e.g., ensemble retriever)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,  # Set the cross-encoder compressor for document reranking\n",
    "    base_retriever=ensemble_retriever  # Use the previously defined ensemble retriever as the base retriever\n",
    ")\n",
    "\n",
    "# Query the retriever and view the compressed document results\n",
    "compressed_docs = compression_retriever.invoke(\"When and where Nike incorporate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike, Inc. was incorporated in 1967 under the laws of the State of Oregon."
     ]
    }
   ],
   "source": [
    "# Define the question for the QA system\n",
    "question = \"When and where was Nike incorporated?\"\n",
    "# Initialize the RetrievalQA chain, configuring it to use the specified LLM and retriever\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # Language model to be used for generating answers\n",
    "    chain_type=\"stuff\",  # Chain type for processing the query and documents\n",
    "    retriever=compression_retriever,  # Retriever to find relevant documents\n",
    "    return_source_documents=False  # Only return the answer, not the source documents\n",
    ")\n",
    "\n",
    "# Function to execute the query with streaming output\n",
    "def run_with_streaming(chain, question):\n",
    "    \"\"\"\n",
    "    Runs the query through the RetrievalQA chain and streams the response in real-time.\n",
    "    :param chain: The QA chain instance.\n",
    "    :param question: The query/question string.\n",
    "    \"\"\"\n",
    "    response = chain.run(question)  # Run the query through the chain\n",
    "    for chunk in response:  # Stream each chunk of the response\n",
    "        print(chunk, end=\"\", flush=True)  # Print each chunk without line breaks for real-time output\n",
    "\n",
    "# Execute the function with streaming enabled\n",
    "run_with_streaming(chain, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
