{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "資料已儲存到 D:\\Project\\2024_POC\\MOL_GPT_0821\\問答集-20240814-測試.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 Excel 檔案\n",
    "excel_file = '' #Excel File path\n",
    "relative_path = '' #txt file path\n",
    "df = pd.read_excel(excel_file)\n",
    "# 儲存為 .txt 檔案，使用 , 或 | 進行區隔\n",
    "with open(relative_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write('|'.join(row.astype(str)) + '\\n')\n",
    "        f.write('-' * 50 + '\\n')  # 用 - 進行區隔\n",
    "\n",
    "print(f'資料已儲存到 {relative_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.55it/s]\n"
     ]
    }
   ],
   "source": [
    "relative_path = '' #File folder path\n",
    "# relative_path = '' #File path\n",
    "\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "loader = DirectoryLoader(relative_path, glob=\"**/*.txt\", show_progress=True,loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "# doc_sources = [doc.metadata['source']  for doc in docs]\n",
    "# loader = TextLoader(relative_path,autodetect_encoding = True)\n",
    "docs = loader.load()\n",
    "# docs = []\n",
    "# for loader in loaders:\n",
    "#     docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap  = 50\n",
    "    )\n",
    "spl_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project\\Virtual_Enviroment\\conda_venv\\HuggingFace\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "#### Most Download model\n",
    "modelPath = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#### BU3 text2vec\n",
    "# modelPath = \"shibing624/text2vec-base-chinese\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cuda'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")\n",
    "# db = FAISS.from_documents(spl_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project\\Virtual_Enviroment\\conda_venv\\HuggingFace\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:168: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://gpt-east-usa.openai.azure.com/ to https://gpt-east-usa.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "d:\\Project\\Virtual_Enviroment\\conda_venv\\HuggingFace\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:175: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "d:\\Project\\Virtual_Enviroment\\conda_venv\\HuggingFace\\lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:183: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://gpt-east-usa.openai.azure.com/ to https://gpt-east-usa.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base = \"\",\n",
    "    openai_api_version=\"\",\n",
    "    azure_deployment=\"\",\n",
    "    openai_api_key=\"\",\n",
    "    openai_api_type=\"\",\n",
    "    temperature = 0.5,\n",
    "    max_tokens = 1024,\n",
    "    streaming=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Retriever\n",
    "The EnsembleRetriever takes a list of retrievers as input and ensemble the results of their get_relevant_documents() methods and rerank the results based on the Reciprocal Rank Fusion algorithm.\n",
    "\n",
    "By leveraging the strengths of different algorithms, the EnsembleRetriever can achieve better performance than any single algorithm.\n",
    "\n",
    "The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as \"hybrid search\". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# doc_list_1 = [\n",
    "#     \"I like apples\",\n",
    "#     \"I like oranges\",\n",
    "#     \"Apples and oranges are fruits\",\n",
    "# ]\n",
    "\n",
    "# # initialize the bm25 retriever and faiss retriever\n",
    "# bm25_retriever = BM25Retriever.from_texts(\n",
    "#     doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
    "# )\n",
    "# bm25_retriever.k = 2\n",
    "\n",
    "# doc_list_2 = [\n",
    "#     \"You like apples\",\n",
    "#     \"You like oranges\",\n",
    "# ]\n",
    "\n",
    "# embedding = embeddings\n",
    "# faiss_vectorstore = FAISS.from_texts(\n",
    "#     doc_list_2, embedding, metadatas=[{\"source\": 2}] * len(doc_list_2)\n",
    "# )\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    spl_text\n",
    ")\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "embedding = embeddings\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    spl_text, embedding\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Encoder Reranker\n",
    "This notebook shows how to implement reranker in a retriever with your own cross encoder from Hugging Face cross encoder models or Hugging Face models that implements cross encoder function (example: BAAI/bge-reranker-base). SagemakerEndpointCrossEncoder enables you to use these HuggingFace models loaded on Sagemaker.\n",
    "\n",
    "This builds on top of ideas in the ContextualCompressionRetriever. Overall structure of this document came from Cohere Reranker documentation.\n",
    "\n",
    "For more about why cross encoder can be used as reranking mechanism in conjunction with embeddings for better retrieval, refer to Hugging Face Cross-Encoders documentation.\n",
    "\n",
    "# Doing reranking with CrossEncoderReranker\n",
    "Now let's wrap our base retriever with a ContextualCompressionRetriever. CrossEncoderReranker uses HuggingFaceCrossEncoder to rerank the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")\n",
    "\n",
    "# 查看查詢出來的文件內容\n",
    "# compressed_docs = compression_retriever.invoke(\"<QUESTION>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果這位38歲的男性計劃開設一家小型咖啡廳並開展外送業務，他需要了解和準備一些重要的申報資訊。以下是一些關鍵的步驟和建議：\n",
      "\n",
      "1. **商業登記**：\n",
      "   - **公司註冊**：確保將咖啡廳註冊為合法的商業實體。可以選擇註冊為獨資經營、合夥企業或有限公司。\n",
      "   - **營業登記**：向當地政府申請營業執照，確保合法經營。\n",
      "\n",
      "2. **稅務申報**：\n",
      "   - **稅務登記**：向稅務部門登記，獲取稅務識別號碼（TIN）。\n",
      "   - **營業稅**：了解當地的營業稅規定，並確保按時申報和繳納。\n",
      "   - **所得稅**：了解個人和企業所得稅的申報要求。\n",
      "\n",
      "3. **衛生和安全許可**：\n",
      "   - **食品衛生許可**：申請食品衛生許可證，確保咖啡廳符合食品安全標準。\n",
      "   - **店鋪設計和設備檢查**：確保店鋪設計和設備符合當地的衛生和安全規範。\n",
      "\n",
      "4. **勞動法規和員工管理**：\n",
      "   - **勞工保險和福利**：為員工提供必要的勞工保險和福利，並遵守勞動法規。\n",
      "   - **員工訓練**：對員工進行必要的培訓，特別是食品安全和客戶服務方面。\n",
      "\n",
      "5. **營運許可和其他相關證照**：\n",
      "   - **營運許可**：根據當地法律，申請其他可能需要的營運許可，例如外送業務的相關許可。\n",
      "   - **廣告和標識許可**：如果需要設置戶外廣告或標識，可能需要申請相關的許可。\n",
      "\n",
      "6. **財務和會計管理**：\n",
      "   - **會計記錄**：保持詳細的財務記錄，確保所有收入和支出都被記錄和報告。\n",
      "   - **財務報表**：定期編制財務報表，了解咖啡廳的經營狀況。\n",
      "\n",
      "7. **市場行銷和推廣**：\n",
      "   - **市場調查**：了解目標市場和競爭對手，制定合適的市場行銷策略。\n",
      "   - **社交媒體和在線推廣**：利用社交媒體和在線平台進行推廣，吸引更多客戶。\n",
      "\n",
      "這些步驟和資訊可以幫助他順利開設和運營咖啡廳，並確保符合所有相關法規和要求。建議他諮詢專業的商業顧問或律師，以獲得更具體和詳細的指導。"
     ]
    }
   ],
   "source": [
    "# docs = ensemble_retriever.invoke(\"RAG\")\n",
    "question = \"這位38歲的男性目前處於失業狀態，已經失業了3到6個月。他在失業後決定自主創業，計劃開設一間小型咖啡廳，目標是為社區居民提供便捷的咖啡服務並開展外送業務。有哪些推薦的申報資訊可以告訴我\"\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=compression_retriever, return_source_documents=False)\n",
    "# print(chain.run(question))\n",
    "# 執行查詢並使用流式輸出\n",
    "def run_with_streaming(chain, question):\n",
    "    response = chain.run(question)\n",
    "    for chunk in response:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "run_with_streaming(chain, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
